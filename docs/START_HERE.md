# START HERE: Content-Addressed Storage Implementation

## ðŸŽ¯ The Goal

Add automatic deduplication to BIRD using content-addressed blob storage.  
**Result:** 70-90% storage savings for CI/CD workloads.

## ðŸ“ Files in This Package

1. **START_HERE.md** (this file) - Quick implementation guide
2. **CONTENT_ADDRESSED_BLOBS.md** - Complete design document
3. **bird_spec_excerpt.md** - Key schema sections

## ðŸš€ Implementation in 3 Days

### Day 1: Schema Migration

**Add new columns:**
```sql
ALTER TABLE outputs ADD COLUMN content_hash TEXT NOT NULL;
ALTER TABLE outputs ADD COLUMN storage_type TEXT NOT NULL DEFAULT 'inline';
ALTER TABLE outputs ADD COLUMN storage_ref TEXT NOT NULL;
```

**Create blob registry:**
```sql
CREATE TABLE blob_registry (
    content_hash      TEXT PRIMARY KEY,
    byte_length       BIGINT NOT NULL,
    ref_count         INT DEFAULT 0,
    first_seen        TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    last_accessed     TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    storage_tier      TEXT,
    storage_path      TEXT,
    verified_at       TIMESTAMP,
    corrupt           BOOLEAN DEFAULT FALSE
);
```

**Add dependencies:**
```toml
[dependencies]
blake3 = "1.5"
flate2 = "1.0"
base64 = "0.21"
```

### Day 2: Capture with Deduplication

```rust
use blake3;
use flate2::write::GzEncoder;
use flate2::Compression;

fn write_output(command_id: &UUID, stream: &str, content: &[u8]) -> Result<()> {
    // 1. Hash the content
    let hash = blake3::hash(content).to_hex().to_string();
    
    // 2. Small outputs: inline as data: URI
    if content.len() < 1_000_000 {
        let b64 = base64::encode(content);
        let storage_ref = format!("data:application/octet-stream;base64,{}", b64);
        
        insert_output_record(OutputRecord {
            command_id,
            stream: stream.to_string(),
            content_hash: hash,
            byte_length: content.len() as i64,
            storage_type: "inline".to_string(),
            storage_ref,
            ..Default::default()
        })?;
        
        return Ok(());
    }
    
    // 3. Large outputs: Check if blob already exists (DEDUPLICATION!)
    let conn = get_db_connection()?;
    
    let existing: Option<String> = conn.query_row(
        "SELECT storage_path FROM blob_registry WHERE content_hash = ?",
        [&hash],
        |row| row.get(0)
    ).optional()?;
    
    let storage_ref = if let Some(path) = existing {
        // DEDUP HIT: Reuse existing blob
        conn.execute(
            "UPDATE blob_registry SET ref_count = ref_count + 1, last_accessed = CURRENT_TIMESTAMP WHERE content_hash = ?",
            [&hash]
        )?;
        
        format!("file://{}", path)
    } else {
        // DEDUP MISS: Write new blob
        let path = write_content_addressed_blob(&hash, content)?;
        
        conn.execute(
            "INSERT INTO blob_registry (content_hash, byte_length, ref_count, storage_tier, storage_path) VALUES (?, ?, 1, 'recent', ?)",
            params![&hash, content.len() as i64, &path]
        )?;
        
        format!("file://{}", path)
    };
    
    insert_output_record(OutputRecord {
        command_id,
        stream: stream.to_string(),
        content_hash: hash,
        byte_length: content.len() as i64,
        storage_type: "blob".to_string(),
        storage_ref,
        ..Default::default()
    })?;
    
    Ok(())
}

fn write_content_addressed_blob(hash: &str, content: &[u8]) -> Result<String> {
    let bird_root = get_bird_root()?;
    
    // Subdirectory: first 2 hex chars (prevents >10k files in one dir)
    let subdir = &hash[..2];
    let blob_dir = bird_root
        .join("db/data/recent/blobs/content")
        .join(subdir);
    
    fs::create_dir_all(&blob_dir)?;
    
    // Filename: {hash}.bin.gz
    let filename = format!("{}.bin.gz", hash);
    let final_path = blob_dir.join(&filename);
    
    // Compress with gzip (DuckDB can read directly)
    let mut encoder = GzEncoder::new(Vec::new(), Compression::default());
    encoder.write_all(content)?;
    let compressed = encoder.finish()?;
    
    // Atomic write (handles concurrent writes of same hash)
    let temp_path = blob_dir.join(format!(".tmp.{}.bin.gz", hash));
    fs::write(&temp_path, compressed)?;
    
    match fs::rename(&temp_path, &final_path) {
        Ok(_) => Ok(format!("recent/blobs/content/{}/{}", subdir, filename)),
        Err(e) if e.kind() == std::io::ErrorKind::AlreadyExists => {
            // Another process wrote same hash - that's fine!
            fs::remove_file(&temp_path).ok();
            Ok(format!("recent/blobs/content/{}/{}", subdir, filename))
        }
        Err(e) => Err(e.into())
    }
}
```

### Day 3: Query with Resolution

```rust
use flate2::read::GzDecoder;

fn resolve_storage_ref(storage_type: &str, storage_ref: &str) -> Result<Vec<u8>> {
    match storage_type {
        "inline" => {
            // Decode data: URI
            if let Some(b64) = storage_ref.strip_prefix("data:application/octet-stream;base64,") {
                Ok(base64::decode(b64)?)
            } else {
                Err(anyhow!("Invalid data: URI"))
            }
        },
        
        "blob" | "archive" => {
            // Read from file:// path
            let rel_path = storage_ref.strip_prefix("file://")
                .ok_or_else(|| anyhow!("Invalid file:// URI"))?;
            
            let bird_root = get_bird_root()?;
            let full_path = bird_root.join("db/data").join(rel_path);
            
            // Decompress gzip
            let file = File::open(full_path)?;
            let mut decoder = GzDecoder::new(file);
            let mut content = Vec::new();
            decoder.read_to_end(&mut content)?;
            
            Ok(content)
        },
        
        _ => Err(anyhow!("Unknown storage type: {}", storage_type))
    }
}

// Update your queries to use storage_ref
fn get_command_output(command_id: &UUID) -> Result<Vec<u8>> {
    let conn = get_db_connection()?;
    
    let (storage_type, storage_ref): (String, String) = conn.query_row(
        "SELECT storage_type, storage_ref FROM outputs WHERE command_id = ? AND stream = 'stdout'",
        [command_id],
        |row| Ok((row.get(0)?, row.get(1)?))
    )?;
    
    resolve_storage_ref(&storage_type, &storage_ref)
}
```

## ðŸ“Š Verify It Works

### Test 1: Same content â†’ Same blob
```bash
# Run identical command twice
shq run echo "test"
shq run echo "test"

# Check deduplication
duckdb ~/.local/share/bird/db/bird.duckdb << EOF
SELECT COUNT(DISTINCT content_hash) as unique_blobs FROM outputs;  -- Should be 1
SELECT COUNT(*) as total_outputs FROM outputs;  -- Should be 2
SELECT ref_count FROM blob_registry;  -- Should be 2
EOF
```

### Test 2: Storage savings
```bash
# Run 100 identical builds
for i in {1..100}; do
  shq run make test
done

# Check storage
du -sh ~/.local/share/bird/db/data/recent/blobs/content/
# Should be ~size of 1 build, not 100!

# Check ref count
duckdb ~/.local/share/bird/db/bird.duckdb << EOF
SELECT content_hash, ref_count FROM blob_registry;  -- Should be ~100
EOF
```

## ðŸŽ¯ Success Criteria

- [ ] 100 identical commands â†’ 1 blob file
- [ ] ref_count correctly tracks references
- [ ] Storage is ~1% of old size (99% savings for identical outputs)
- [ ] All existing queries still work
- [ ] No performance regression (<3ms overhead)

## ðŸ’¡ The Magic

The entire deduplication happens in these 2 lines:

```rust
// Check if blob exists
if let Some(path) = blob_exists(&hash)? {
    increment_ref_count(&hash)?;  // REUSE! ðŸŽ‰
} else {
    write_blob(&hash, content)?;  // Write new
}
```

That's it! Content-addressed storage saves 70-90% storage with minimal code.

## ðŸ“ Directory Structure

```
$BIRD_ROOT/db/data/
â”œâ”€â”€ recent/
â”‚   â”œâ”€â”€ commands/
â”‚   â”‚   â””â”€â”€ date=YYYY-MM-DD/*.parquet
â”‚   â”œâ”€â”€ outputs/
â”‚   â”‚   â””â”€â”€ date=YYYY-MM-DD/*.parquet
â”‚   â””â”€â”€ blobs/
â”‚       â””â”€â”€ content/              # Content-addressed pool
â”‚           â”œâ”€â”€ 00/
â”‚           â”‚   â””â”€â”€ 00abc123...def.bin.gz
â”‚           â”œâ”€â”€ 01/
â”‚           â”œâ”€â”€ ...
â”‚           â””â”€â”€ ff/               # 256 subdirs total
â””â”€â”€ archive/
    â””â”€â”€ blobs/
        â””â”€â”€ content/              # Global pool (not date-partitioned)
```

## ðŸ”§ Troubleshooting

### "Hash mismatch"
- Corruption in blob file
- Check: `SELECT * FROM blob_registry WHERE corrupt = TRUE`
- Recompute hash or delete

### "Dedup not working"
- Verify hashes: `SELECT content_hash, COUNT(*) FROM outputs GROUP BY content_hash`
- Check BLAKE3 version consistency
- Log dedup hit/miss rates

### "Too many files in directory"
- Should have 256 subdirs (00-ff)
- Check: `ls -1 blobs/content/ | wc -l` should be 256
- Each subdir has ~N/256 files

## ðŸ“ˆ Expected Results

| Workload | Commands/Day | Avg Output | Dedup Ratio | Before | After | Savings |
|----------|-------------|------------|-------------|--------|-------|---------|
| CI Tests | 10,000 | 200KB | 80% | 60GB | 12GB | **48GB (80%)** |
| Builds | 1,000 | 5MB | 70% | 150GB | 45GB | **105GB (70%)** |
| Local Dev | 5,000 | 100KB | 60% | 15GB | 6GB | **9GB (60%)** |

## ðŸš€ Deployment

1. **Backup existing data**
   ```bash
   cp -r ~/.local/share/bird ~/.local/share/bird.backup
   ```

2. **Run schema migration**
   ```bash
   shq migrate schema-v2
   ```

3. **Deploy new code**
   ```bash
   cargo build --release
   cp target/release/shq ~/.local/bin/
   ```

4. **Monitor dedup rate**
   ```bash
   duckdb bird.duckdb << EOF
   SELECT 
     COUNT(*) as total_outputs,
     COUNT(DISTINCT content_hash) as unique_blobs,
     ROUND(100.0 * (1 - COUNT(DISTINCT content_hash)::FLOAT / COUNT(*)), 1) as dedup_percent
   FROM outputs;
   EOF
   ```

## ðŸ“š Additional Reading

See the other files in this package:
- **CONTENT_ADDRESSED_BLOBS.md** - Full design rationale
- **bird_spec_excerpt.md** - Complete schema definitions

---

**Time to implement:** 3-4 days  
**Complexity:** Medium  
**Risk:** Low (backward compatible)  
**Reward:** 70-90% storage reduction ðŸŽ¯
